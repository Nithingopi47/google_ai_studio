{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOiOJ2iz9wHZ7zcdr9eby4o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nithingopi47/google_ai_studio/blob/main/Gemini_2_0_Flash_Thinking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gemini 2.0 Flash Thinking"
      ],
      "metadata": {
        "id": "rMwmspDF1QxD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Gemini 2.0 Flash Thinking model is an experimental model that's trained to generate the \"thinking process\" the model goes through as part of its response. As a result, the Flash Thinking model is capable of stronger reasoning capabilities in its responses than the Gemini 2.0 Flash Experimental model.\n",
        "\n"
      ],
      "metadata": {
        "id": "WfVnlsUz1T9M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use thinking models"
      ],
      "metadata": {
        "id": "KMiGFmL71hfe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Flash Thinking models are available in Google AI Studio and through the Gemini API. The Gemini API doesn't return thoughts in the response."
      ],
      "metadata": {
        "id": "bwRmb6PO1V1M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: We have set up gemini-2.0-flash-thinking-exp as an alias to the latest Flash Thinking model. Use this alias to get the latest Flash thinking model, or specify the full model name."
      ],
      "metadata": {
        "id": "lGDJ3B-R1oMT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Send a basic request"
      ],
      "metadata": {
        "id": "m5rJ2JKQ1sXq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This example uses the new Google Genai SDK and the v1alpha version of the API."
      ],
      "metadata": {
        "id": "FLIbwL-C2R2s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNgLHxf_0BVc",
        "outputId": "f3b7a61f-89fa-4e2d-eabe-5bd834b65237"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imagine you're training a puppy to be a well-behaved dog.  That's kind of what RLHF is doing for AI, specifically for language models (like the ones that power chatbots).\n",
            "\n",
            "Let's break it down:\n",
            "\n",
            "**The Problem: AI Can Be Smart, But Not Always \"Good\"**\n",
            "\n",
            "Think of a really smart puppy. It can learn tricks quickly (like generating text), but it might also:\n",
            "\n",
            "* **Be too creative (make things up):**  Like a puppy chewing on your shoes because it thinks it's fun, AI can generate text that sounds good but is factually wrong or nonsensical.\n",
            "* **Be unhelpful (ignore your commands):**  If you ask the AI to be concise, it might give you a long, rambling answer.\n",
            "* **Be even *bad* (bite or bark aggressively):** AI could generate harmful, biased, or offensive text.\n",
            "\n",
            "Basically, the AI is good at *generating text*, but it's not naturally good at generating *helpful, harmless, and honest* text â€“ the kind of text humans actually want.\n",
            "\n",
            "**RLHF: Teaching AI to Be \"Good\" with Human Feedback**\n",
            "\n",
            "RLHF (Reinforcement Learning from Human Feedback) is like teaching your puppy good manners using rewards and corrections. It's a process with these main steps:\n",
            "\n",
            "**1. Initial Training (Puppy Learns Basic Tricks):**\n",
            "\n",
            "* **Pre-training:** First, the AI is trained on a massive amount of text data from the internet. This is like teaching the puppy basic commands like \"sit,\" \"stay,\" and \"fetch\" using lots of examples.  The AI learns to predict the next word in a sentence and gets good at generating text that sounds grammatically correct.\n",
            "\n",
            "**2. Supervised Fine-tuning (Showing the Puppy What \"Good\" Looks Like):**\n",
            "\n",
            "* **Curated Examples:** Next, humans (like dog trainers) give the AI examples of good conversations or answers.  They might write questions and then provide ideal, well-written answers.  This is like showing the puppy exactly what \"sit politely\" looks like. The AI learns to mimic this style.\n",
            "\n",
            "**3. Building a \"Taste Tester\" (Training a Judge for Good Dog Behavior):**\n",
            "\n",
            "* **Reward Model Training:** This is the core of RLHF.  Instead of just showing examples, we now want the AI to learn what *humans prefer*.\n",
            "    * **Human Ranking:** Humans are asked to compare different responses the AI generates to the *same* question. They rank them from best to worst based on qualities like helpfulness, truthfulness, and harmlessness.  Imagine humans saying \"Response A is better than Response B, which is better than Response C.\"\n",
            "    * **Reward Model:** This ranking data is used to train a *separate* AI model called the \"Reward Model.\" This model learns to predict how humans would rank different responses. It becomes a \"taste tester\" for good text.  It learns to give a \"reward score\" to different AI responses, with higher scores for responses humans prefer.  Think of it like the puppy learning to recognize when you are happy with its behavior.\n",
            "\n",
            "**4. Reinforcement Learning (Training the Puppy to Get Rewards):**\n",
            "\n",
            "* **Optimization with Rewards:** Now, the original language model (the puppy) is trained using reinforcement learning.\n",
            "    * **AI Tries, Gets Feedback:** The AI generates text responses.\n",
            "    * **Reward Model Judges:** The \"Reward Model\" (the taste tester) evaluates these responses and gives them a reward score.\n",
            "    * **AI Learns to Maximize Rewards:** The AI adjusts its way of generating text to try and get higher reward scores from the Reward Model. It's like the puppy learning to do things that make you happy and give it treats.  It learns to generate text that the Reward Model (and therefore, hopefully, humans) will like.\n",
            "\n",
            "**In Simple Analogy - The Restaurant Critic:**\n",
            "\n",
            "Imagine a restaurant (the language model) trying to get good reviews.\n",
            "\n",
            "1. **Basic Cooking Skills (Pre-training):** The restaurant learns basic recipes from cookbooks (internet text).\n",
            "2. **Chef Training (Supervised Fine-tuning):**  Famous chefs give the restaurant specific recipes and instructions (curated examples).\n",
            "3. **Hiring a Food Critic (Reward Model):**  You hire a food critic (Reward Model) to taste the dishes and give scores based on what customers (humans) like (taste, presentation, etc.). The critic learns what good food is based on customer feedback.\n",
            "4. **Improving the Menu Based on Reviews (Reinforcement Learning):**  The restaurant changes its recipes and menu based on the critic's scores to get better reviews and please customers.\n",
            "\n",
            "**Why is RLHF Important?**\n",
            "\n",
            "RLHF helps make AI more:\n",
            "\n",
            "* **Helpful:**  Answers questions better, provides useful information.\n",
            "* **Harmless:**  Avoids generating offensive, biased, or dangerous content.\n",
            "* **Honest:**  Sticks to facts, avoids making things up (as much as possible).\n",
            "* **Aligned with Human Values:**  Reflects what humans generally consider good and desirable in AI behavior.\n",
            "\n",
            "**In short, RLHF is like teaching AI good manners by getting human feedback on what \"good\" behavior looks like and then training the AI to maximize that \"goodness.\"** It's a crucial step in making AI assistants more useful and safe for everyone.\n"
          ]
        }
      ],
      "source": [
        "from google import genai\n",
        "\n",
        "client = genai.Client(api_key='GEMINI_API_KEY', http_options={'api_version':'v1alpha'})\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash-thinking-exp',\n",
        "    contents='Explain how RLHF works in simple terms.',\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-turn thinking conversations"
      ],
      "metadata": {
        "id": "eDgB2Khq2CKb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "During multi-turn conversations, you pass the entire conversation history as input, so the model has access to its previous thoughts in a multi-turn conversation."
      ],
      "metadata": {
        "id": "0RYC65KT2FiE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The new Google Genai SDK provides the ability to create a multi-turn chat session which is helpful to manage the state of a conversation.\n",
        "\n"
      ],
      "metadata": {
        "id": "qvYJh-HA229c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "\n",
        "client = genai.Client(api_key='GEMINI_API_KEY', http_options={'api_version':'v1alpha'})\n",
        "\n",
        "chat = client.aio.chats.create(\n",
        "    model='gemini-2.0-flash-thinking-exp',\n",
        ")\n",
        "response = await chat.send_message('What is your name?')\n",
        "print(response.text)\n",
        "response = await chat.send_message('What did you just say before this?')\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRGtn3mP2Exk",
        "outputId": "036bf025-9fd3-4360-f0cd-43ef6f442c78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I don't have a name in the way humans do.  You can think of me as a large language model, trained by Google.\n",
            "Just before this, I said: \"I don't have a name in the way humans do. You can think of me as a large language model, trained by Google.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Limitations"
      ],
      "metadata": {
        "id": "YhRukiLx3aWp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Flash Thinking model is an experimental model and has the following limitations:\n",
        "\n",
        "*   No JSON mode or Search Grounding\n",
        "*   Thoughts are only shown in Google AI Studio\n"
      ],
      "metadata": {
        "id": "IKfAQOah3fAg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What's next?"
      ],
      "metadata": {
        "id": "8k5KEThb3i_-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Try the Flash Thinking model in Google AI Studio.\n",
        "*   Try the Flash Thinking Colab\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kKEgy63q3mN3"
      }
    }
  ]
}