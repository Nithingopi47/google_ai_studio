{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqQgWvcYCP+pDsVziawbpQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nithingopi47/google_ai_studio/blob/main/Multimodal_Live_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multimodal Live API"
      ],
      "metadata": {
        "id": "dA7hGnuH_5MK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Multimodal Live API enables low-latency bidirectional voice and video interactions with Gemini. Using the Multimodal Live API, you can provide end users with the experience of natural, human-like voice conversations, and with the ability to interrupt the model's responses using voice commands. The model can process text, audio, and video input, and it can provide text and audio output."
      ],
      "metadata": {
        "id": "wFL8FHLH_-uo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Capabilities\n",
        "Multimodal Live API includes the following key capabilities:\n",
        "\n",
        "*   Multimodality: The model can see, hear, and speak.\n",
        "\n",
        "*   Low-latency real-time interaction: Provides fast responses\n",
        "\n",
        "*   Session memory: The model retains memory of all interactions within a single session, recalling previously heard or seen information.\n",
        "\n",
        "*   Support for function calling, code execution, and Search as a tool: Enables integration with external services and data sources.\n",
        "*   Automated voice activity detection (VAD): The model can accurately recognize when the user begins and stops speaking. This allows for natural, conversational interactions and empowers users to interrupt the model at any time.\n",
        "You can try the Multimodal Live API in Google AI Studio.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0QbMrturAnSS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get started\n",
        "\n",
        "Multimodal Live API is a stateful API that uses WebSockets.\n",
        "\n",
        "This section shows an example of how to use Multimodal Live API for text-to-text generation, using Python 3.9+"
      ],
      "metadata": {
        "id": "QTWlqQzqBJKm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install the Gemini API library\n",
        "To install the google-genai package, use the following pip command:"
      ],
      "metadata": {
        "id": "rEZ1skZFBuZl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1a8vwhMG_yl7",
        "outputId": "96922978-c00c-4a31-dc81-47d9e4bf8e5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-genai in /usr/local/lib/python3.11/dist-packages (0.8.0)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.27.0)\n",
            "Requirement already satisfied: pydantic<3.0.0dev,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.10.6)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.28.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.32.3)\n",
            "Requirement already satisfied: websockets<15.0dev,>=13.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (14.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-genai) (5.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-genai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-genai) (4.9)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0dev,>=2.0.0->google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0dev,>=2.0.0->google-genai) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0dev,>=2.0.0->google-genai) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.28.1->google-genai) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.28.1->google-genai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.28.1->google-genai) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.28.1->google-genai) (2025.1.31)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-genai) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "pip install google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import dependencies\n",
        "To import dependencies:"
      ],
      "metadata": {
        "id": "XiZqZA-8CG4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbl7DozRCKmT",
        "outputId": "5af0db16-448d-4c9a-97cc-5898a9eefd49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:502: UserWarning: <built-in function any> is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.\n",
            "  warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Send and receive a text message"
      ],
      "metadata": {
        "id": "-zzvMg8nCMev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import nest_asyncio\n",
        "from google import genai\n",
        "\n",
        "# Apply nest_asyncio to allow nested event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "client = genai.Client(api_key=\"Gemini_api_key\", http_options={'api_version': 'v1alpha'})\n",
        "model_id = \"gemini-2.0-flash-exp\"\n",
        "config = {\"response_modalities\": [\"TEXT\"]}\n",
        "\n",
        "async def main():\n",
        "    # The main async function where the connection to the live API is made\n",
        "    async with client.aio.live.connect(model=model_id, config=config) as session:\n",
        "        while True:\n",
        "            message = input(\"User> \")\n",
        "            if message.lower() == \"exit\":\n",
        "                break\n",
        "            await session.send(input=message, end_of_turn=True)\n",
        "\n",
        "            async for response in session.receive():\n",
        "                if response.text is None:\n",
        "                    continue\n",
        "                print(response.text, end=\"\")\n",
        "\n",
        "async def wrapper():\n",
        "    # This function will wrap main() so that it can be run in a pre-existing event loop.\n",
        "    await main()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Instead of asyncio.run(), use asyncio.create_task() to schedule the async task\n",
        "    # to run in the existing loop.\n",
        "    asyncio.create_task(wrapper())\n",
        "    # Let the loop run indefinitely.\n",
        "    loop = asyncio.get_event_loop()\n",
        "    loop.run_forever()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "VpeT26y9CQJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the required library\n",
        "!pip install nest_asyncio"
      ],
      "metadata": {
        "id": "ASyBjTCvWATo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Integration guide\n",
        "This section describes how integration works with Multimodal Live API.\n",
        "\n",
        "# Sessions\n",
        "A WebSocket connection establishes a session between the client and the Gemini server.\n",
        "\n",
        "After a client initiates a new connection the session can exchange messages with the server to:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   LSend text, audio, or video to the Gemini server.\n",
        "*   Receive audio, text, or function call requests from the Gemini server.\n",
        "\n",
        "\n",
        "The session configuration is sent in the first message after connection. A session configuration includes the model, generation parameters, system instructions, and tools.\n",
        "\n",
        "See the following example configuration. Note that the name casing in SDKs may vary. You can look up the Python SDK configuration options here.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XeAcSfVZCTS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"model\": string,\n",
        "  \"generationConfig\": {\n",
        "    \"candidateCount\": integer,\n",
        "    \"maxOutputTokens\": integer,\n",
        "    \"temperature\": number,\n",
        "    \"topP\": number,\n",
        "    \"topK\": integer,\n",
        "    \"presencePenalty\": number,\n",
        "    \"frequencyPenalty\": number,\n",
        "    \"responseModalities\": [string],\n",
        "    \"speechConfig\": object\n",
        "  },\n",
        "  \"systemInstruction\": string,\n",
        "  \"tools\": [object]\n",
        "}"
      ],
      "metadata": {
        "id": "S726QzKDCiCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For more information, see BidiGenerateContentSetup."
      ],
      "metadata": {
        "id": "-SmHSqSVCnlV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Send messages\n",
        "Messages are JSON-formatted objects exchanged over the WebSocket connection.\n",
        "\n",
        "To send a message the client must send a JSON object over an open WebSocket connection. The JSON object must have exactly one of the fields from the following object set:"
      ],
      "metadata": {
        "id": "PM-keSf9Coa-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"setup\": BidiGenerateContentSetup,\n",
        "  \"clientContent\": BidiGenerateContentClientContent,\n",
        "  \"realtimeInput\": BidiGenerateContentRealtimeInput,\n",
        "  \"toolResponse\": BidiGenerateContentToolResponse\n",
        "}\n"
      ],
      "metadata": {
        "id": "TytcKQSbCsyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Supported client messages\n",
        "See the supported client messages in the following table:\n",
        "\n",
        "  \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Message\t: BidiGenerateContentSetup\n",
        "\n",
        "Description : Session configuration to be sent in the first message\n",
        "\n",
        "Message\t: BidiGenerateContentClientContent\n",
        "\n",
        "Description : Incremental content update of the current conversation delivered from the client\n",
        "\n",
        "Message\t: BidiGenerateContentRealtimeInput\n",
        "\n",
        "Description : Real time audio or video input\n",
        "\n",
        "Message\t: BidiGenerateContentToolResponse\n",
        "\n",
        "Description : Response to a ToolCallMessage received from the server\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "gnU2zuSfC16I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Receive messages\n",
        "To receive messages from Gemini, listen for the WebSocket 'message' event, and then parse the result according to the definition of the supported server messages.\n",
        "\n",
        "See the following:"
      ],
      "metadata": {
        "id": "aBEoQdNpC8rZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Python code does not support JavaScript syntax.\n",
        "# To process websocket data in Python, you'll need a different approach using a library like websockets.\n",
        "# The following is an example of handling binary or text data received over a websocket in Python:\n",
        "\n",
        "\n",
        "import asyncio\n",
        "import websockets\n",
        "\n",
        "async def handler(websocket):\n",
        "    async for message in websocket:\n",
        "        if isinstance(message, bytes):\n",
        "            # Process binary data (audio, video)\n",
        "            print(\"Received binary data:\", message)\n",
        "        else:\n",
        "            # Process JSON or text response\n",
        "            print(\"Received text data:\", message)\n",
        "\n",
        "async def main():\n",
        "    async with websockets.serve(handler, \"localhost\", 8765):\n",
        "        await asyncio.Future()  # Run forever\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())"
      ],
      "metadata": {
        "id": "P3OojD5NEFIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Server messages will have exactly one of the fields from the following object set:"
      ],
      "metadata": {
        "id": "eACzvY74EIUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"setupComplete\": BidiGenerateContentSetupComplete,\n",
        "  \"serverContent\": BidiGenerateContentServerContent,\n",
        "  \"toolCall\": BidiGenerateContentToolCall,\n",
        "  \"toolCallCancellation\": BidiGenerateContentToolCallCancellation\n",
        "}"
      ],
      "metadata": {
        "id": "2h8FsOb_EKZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Supported server messages\n",
        "\n",
        "Message : BidiGenerateContentSetupComplete\n",
        "\n",
        "Description : A BidiGenerateContentSetup message from the client, sent when setup is complete\n",
        "\n",
        "Message : BidiGenerateContentServerContent\n",
        "\n",
        "Description : Content generated by the model in response to a client message\n",
        "\n",
        "Message : BidiGenerateContentToolCall\n",
        "\n",
        "Description : Request for the client to run the function calls and return the responses with the matching IDs\n",
        "\n",
        "Message : BidiGenerateContentToolCallCancellation\n",
        "\n",
        "Description : Sent when a function call is canceled due to the user interrupting model output\n"
      ],
      "metadata": {
        "id": "Er46IHzwEPVT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Incremental content updates\n",
        "Use incremental updates to send text input, establish session context, or restore session context. For short contexts you can send turn-by-turn interactions to represent the exact sequence of events. For longer contexts it's recommended to provide a single message summary to free up the context window for the follow up interactions.\n",
        "\n",
        "See the following example context message:"
      ],
      "metadata": {
        "id": "m-Ee059sHywt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"clientContent\": {\n",
        "    \"turns\": [\n",
        "      {\n",
        "          \"parts\":[\n",
        "          {\n",
        "            \"text\": \"\"\n",
        "          }\n",
        "        ],\n",
        "        \"role\":\"user\"\n",
        "      },\n",
        "      {\n",
        "          \"parts\":[\n",
        "          {\n",
        "            \"text\": \"\"\n",
        "          }\n",
        "        ],\n",
        "        \"role\":\"model\"\n",
        "      }\n",
        "    ],\n",
        "    \"turnComplete\": true\n",
        "  }\n",
        "}"
      ],
      "metadata": {
        "id": "UgZGjnzqH1pJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that while content parts can be of a functionResponse type, BidiGenerateContentClientContent shouldn't be used to provide a response to the function calls issued by the model. BidiGenerateContentToolResponse should be used instead. BidiGenerateContentClientContent should only be used to establish previous context or provide text input to the conversation."
      ],
      "metadata": {
        "id": "HoCsDeYjH35X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Streaming audio and video"
      ],
      "metadata": {
        "id": "x9Ww_TuvH7Fs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function calling\n",
        "All functions must be declared at the start of the session by sending tool definitions as part of the BidiGenerateContentSetup message.\n",
        "\n",
        "See the Function calling tutorial to learn more about function calling.\n",
        "\n",
        "From a single prompt, the model can generate multiple function calls and the code necessary to chain their outputs. This code executes in a sandbox environment, generating subsequent BidiGenerateContentToolCall messages. The execution pauses until the results of each function call are available, which ensures sequential processing.\n",
        "\n",
        "The client should respond with BidiGenerateContentToolResponse.\n",
        "\n",
        "Audio inputs and audio outputs negatively impact the model's ability to use function calling."
      ],
      "metadata": {
        "id": "Zg1SI80wH8VY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Audio formats\n",
        "Multimodal Live API supports the following audio formats:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Input audio format: Raw 16 bit PCM audio at 16kHz little-endian\n",
        "*   Output audio format: Raw 16 bit PCM audio at 24kHz little-endian\n"
      ],
      "metadata": {
        "id": "JUnRzheLIGry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# System instructions\n",
        "You can provide system instructions to better control the model's output and specify the tone and sentiment of audio responses.\n",
        "\n",
        "System instructions are added to the prompt before the interaction begins and remain in effect for the entire session.\n",
        "\n",
        "System instructions can only be set at the beginning of a session, immediately following the initial connection. To provide further input to the model during the session, use incremental content updates."
      ],
      "metadata": {
        "id": "eo2ndohdJtp_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interruptions\n",
        "Users can interrupt the model's output at any time. When Voice activity detection (VAD) detects an interruption, the ongoing generation is canceled and discarded. Only the information already sent to the client is retained in the session history. The server then sends a BidiGenerateContentServerContent message to report the interruption.\n",
        "\n",
        "In addition, the Gemini server discards any pending function calls and sends a BidiGenerateContentServerContent message with the IDs of the canceled calls."
      ],
      "metadata": {
        "id": "wtfeR_a2JxdX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Voices\n",
        "Multimodal Live API supports the following voices: Aoede, Charon, Fenrir, Kore, and Puck.\n",
        "\n",
        "To specify a voice, set the voiceName within the speechConfig object, as part of your session configuration.\n",
        "\n",
        "See the following JSON representation of a speechConfig object:"
      ],
      "metadata": {
        "id": "xjeZ0QvmJzA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"voiceConfig\": {\n",
        "    \"prebuiltVoiceConfig\": {\n",
        "      \"voiceName\": \"VOICE_NAME\"\n",
        "    }\n",
        "  }\n",
        "}"
      ],
      "metadata": {
        "id": "vBHUP4QoJ3k4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Limitations\n",
        "Consider the following limitations of Multimodal Live API and Gemini 2.0 when you plan your project."
      ],
      "metadata": {
        "id": "F78acPX4J18x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Client authentication\n",
        "Multimodal Live API only provides server to server authentication and isn't recommended for direct client use. Client input should be routed through an intermediate application server for secure authentication with the Multimodal Live API.\n",
        "\n",
        "For web and mobile app deployments, you can explore options from:\n",
        "\n",
        "\n",
        "\n",
        "*   Daily\n",
        "*   Livekit\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Aky3JKW8KAJz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conversation history\n",
        "While the model keeps track of in-session interactions, conversation history isn't stored. When a session ends, the corresponding context is erased.\n",
        "\n",
        "In order to restore a previous session or provide the model with historic context of user interactions, the application should maintain its own conversation log and use a BidiGenerateContentClientContent message to send this information at the start of a new session."
      ],
      "metadata": {
        "id": "l1UV-VwmKH-U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Maximum session duration\n",
        "Session duration is limited to up to 15 minutes for audio or up to 2 minutes of audio and video. When the session duration exceeds the limit, the connection is terminated.\n",
        "\n",
        "The model is also limited by the context size. Sending large chunks of content alongside the video and audio streams may result in earlier session termination."
      ],
      "metadata": {
        "id": "ulk-4-s8KPbe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Voice activity detection (VAD)\n",
        "The model automatically performs voice activity detection (VAD) on a continuous audio input stream. VAD is always enabled, and its parameters aren't configurable."
      ],
      "metadata": {
        "id": "iOuv-jbyKTWN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Token count\n",
        "Token count isn't supported."
      ],
      "metadata": {
        "id": "EtPGPqGHLWFG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rate limits\n",
        "The following rate limits apply:\n",
        "\n",
        "3 concurrent sessions per API key\n",
        "4M tokens per minute"
      ],
      "metadata": {
        "id": "UQNyu2WDLao_"
      }
    }
  ]
}